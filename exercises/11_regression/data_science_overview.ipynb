{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some thoughts on data science \n",
    "\n",
    "## introduction\n",
    "**data science** is a nebulous term which means different things to different people. there are disagreements out there as to what data science is and is not. almost nothing can be said about data science that someone, somewhere won't passionately disagree with. what follows is thus just my interpretation, opinion, and prejudice.\n",
    "\n",
    "since the word **science** occurs in it, data science must have something to do **with generating new knowledge**, by asking, considering, and answering **questions**. additionally, the answers we find must be supported by the evidence we find in some **data** (as opposed to applications of *first principles* and/or *logical reasoning* alone). so now the attention shifts to the **questions** that data science deals with.\n",
    "\n",
    "now, to be clear: there are other tasks involved in the work of a data scientist that do not exactly fall under the rubric of answering research questions (e.g. *collecting data*, *designing experiments*, *learning and evaluating the latest algoritms*, *project management*, *giving presentations*, etc.) but these are all done on theperphery of, and in support of the _ultimate goal_ of answering questions, so let us put those details aside for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the questions\n",
    "\n",
    "the questions that data scientists get tasked to consider are extremely varied (which is why data science is *the most fun job* ever invented). yet, all the diversity of data science questions can be boiled down to just **5 fundamental questions**. at a high level, all our projects can be described as an attempt to answer one or more (usually more!) of these.\n",
    "\n",
    "\n",
    "## the fundamental questions data scientists think about\n",
    "each of the questions has a technical name but informally stated, they are:\n",
    "\n",
    "- **how much is it**? (technical term: regression/forecasting)\n",
    "- **which type is it**? (technical term: classification)\n",
    "- **what's in it**? (technical term: clustering)\n",
    "- **what's weird**? (technical term: anomaly detection)\n",
    "- **what's next**? (technical term: automated decision making)\n",
    "\n",
    "for most questions that we come face to face with there already exist a set of standard methods, called algorihtms, to address them. \n",
    "\n",
    "### how much is it? \n",
    "these questions have a numerical answer. examples include \"what will next quarter's customer churn be?\", \"how many likes would this kind of post (most likely) get?\", or \"what is the (average) time between events of this type?\" \n",
    "\n",
    "we answer these kinds of questions by fitting a *quantitative model* (a concrete mathematical description) to the data and computing its prediction for a given input. how difficult it is to answer this kind question depending on the applicable model. \n",
    "\n",
    "often, this model fitting is of a type called regression (the process of estimating numerical relationship between variables). the most familiar form of regression is linear regressions, the drawing of a straight line fit through a set of points on a scatter plot. generalised linear model regression is the same but does not assume the normally distributed noise signal. polynomial fittinga and exponential fitting methods can be linearised in the coefficients. \n",
    "\n",
    "in addition, this kind of questions may require is non-linear regression which is addressed by approximation methods or may require the full heavy machinery of advanced time series analysis and signal processing.\n",
    "\n",
    "thus, this kind of question may be the easiest data science problem there is, or the hardest.\n",
    "\n",
    "\n",
    "### which type is it?\n",
    "these questions involve classification, the placing of a set of objects into (known) categories with clear boundaries. the simplest example is a binary choice: \"is x of type a or type b?\",  or have multiple categories: \"which country is x from?\"\n",
    "\n",
    "these kind of questions can sometimes be phrased differently: \"does this mention contain sadness?\", or \"what emotions are conveyed by this mention?\", \"does this diagnostic x-ray show signs of illness y?\". \n",
    "\n",
    "assuming the data we have contains example observations of each the categories we have a machine learning problem  called a *supervised learning* problem. note that the underlying *qualitative model* does not allow for ambiguity in which category an observation fits.\n",
    "\n",
    "logistic regression can help answer these kind of questions. \n",
    "\n",
    "\n",
    "### what's in it?\n",
    "we ask this question when we need to break a set of observations up into segments that have more in common internally than between segments. what \"more in common\" means precisely varies from case to case and can be hard to formulate. \n",
    "examples include \"what sub-populations exist in my set of customers?\" and \"are all of these observations of the same type or are there distinct types among them?\"\n",
    "\n",
    "this question is a sibling of the classification question, but is one that applies when the categories that the observations can fall into are not known in advance, and when the categories have soft, indeterminate boundaries.\n",
    "\n",
    "the technical term for answering questions like these is *clustering*. an in machine learning parlance it is an *unsupervised learning* problem. \n",
    "\n",
    "\n",
    "### is this weird? \n",
    "this type of question involves comparison between realised cases and stated expectations, to find which cases violate  our assumptions. as such this question can be thought as a form of classification ('expected' vs. 'not expected') but the difference lies in how less determined definition of the categories is. by definition we do not have a complete, unambiguous description of what the unexpected event looks like. \n",
    "\n",
    "examples include \"which data points are outliers?\", \"is this credit card transaction genuine or fraudulent?\", \"is this social media user a human or a bot?\", \"is this a regular user logged in to my server or is it a hacker?\"\n",
    "\n",
    "the technical term for this question is anomaly detection and it is often answered by some kind of statistical tests. \n",
    "\n",
    "- *point anomalies* are soemtimes found by setting a threshold, removing noise by applying smoothing, and filtering values outside of the threshold. \n",
    "- *contextual anomalies* are found by defining a full set of allowed conditions and testing all events against those. \n",
    "- *collective anomalies* are found by grouping events together and \n",
    "\n",
    "in case i made it sound boring by the sterile text above, let me add that these kinds of questions are super fun to tackle! they are hard, involving real-life detective work and lateral thinking. the effort can lead to delightfully surprising results. \n",
    "\n",
    "\n",
    "### what's next? \n",
    "these questions involve choosing one of several available options while optimising some function (cost/benefit). examples: \"(given what people purchase together and what current shopper has in the basket) which products should we recommend (to maximise odds of them buying it)\"? \"which mention (out of this large set) should we present to the client first (as the most representative given a certain topic)?\".\n",
    "\n",
    "the technical term is automated decision making, and the methods used to answer this may include reinforcement learning. self-driving cars, chess playing computers, and robot vacuum all use these same technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## machine learning\n",
    "*machine learning* refers to the art  of applying data science algorithms to address the questions. generally, the steps are:\n",
    "\n",
    "- design a study,\n",
    "- collect the raw data\n",
    "- clean (and balance, if needed) the data set\n",
    "- split the full data set into training set(s) and testing set(s)\n",
    "- select a set of hyper parameters\n",
    "- *train* a model on the training set (get estimates for the model parameters)\n",
    "- using model parameter estimates, predict the values of the test set.\n",
    "- compare the predictions of the model against the actual observations of test set to gauge success (see also statistical testing).\n",
    "- as needed, repeat last previous steps, tune hyper parameters, retrain model, and compare, till you have an acceptable confidence in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statistical testing\n",
    "\n",
    "*statistical testing* refers to the rigorous methodology used to determine to what degree we believe in our quantitative models. there are two main schools of thought within this discipline, bayesian and frequentist. *hypothesis testing* is probably the most common format of statistical testing. we will discuss in detail later, but the basic steps are:\n",
    "\n",
    "- use the research question to formulate a hypothesis to test. this is your $H_1$. the hypothesis makes a claim about some relationship between variables. (e.g. $H_1$ states that $a$ is different from $b$).\n",
    "- negate that hypothesis by its opposite, called the null hypothesis, $H_0$. (e.g. $H_0$ states that there is no difference between some $a$, and $b$). the rest of the process is to determine whether the evidence is strong enough to reject the null hypothesis. \n",
    "- given the hypothesis, devise a test statistic, which is a number that depends on the input data whose statistical distribution you know in the case when the null hypothesis is true. \n",
    "- decide on a non-zero, acceptable false positive rate (typically $\\alpha =5\\%$ or $\\alpha =1\\%$). its complement is called the $p$-value: $p=1-\\alpha$\n",
    "- collect relevant, unbiased, and clean data (hah!)\n",
    "- calculate a test statistics, which is a number which depends on the input data whose statistical distribution you know in the case when the null hypothesis is true. \n",
    "- calculate where on its distribution the computed value of the test statistic is, given the data. \n",
    "- calculate the probability of having collected the dataset you have, if the null hypothesis is true. \n",
    "- compare this probability with the p-value to decide whether to reject the null hypothesis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
