{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/\n"
     ]
    }
   ],
   "source": [
    "# Defining directory where files are stored\n",
    "LZ_HOME = os.path.expanduser('~')\n",
    "file_dir = os.path.join(LZ_HOME, 'LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/')\n",
    "print(file_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/example_filters.csv\n"
     ]
    }
   ],
   "source": [
    "filename_csv = os.path.join(file_dir, \"example_filters.csv\")\n",
    "print(filename_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: False\n",
      "Delimiter: e\n",
      "Escape char: None\n",
      "Doublequote: False\n",
      "Quote char: \"\n",
      "Line terminator: \r\n",
      "\n",
      "Dialect quoting: 0\n",
      "Skip initial space: False\n"
     ]
    }
   ],
   "source": [
    "with open(filename_csv, newline='') as infile:\n",
    "    sample = infile.read(24) \n",
    "    inspector_object = csv.Sniffer()\n",
    "    print(\"Header:\", inspector_object.has_header(sample))\n",
    "    dialect = inspector_object.sniff(sample)\n",
    "    print(\"Delimiter:\", dialect.delimiter)\n",
    "    print(\"Escape char:\", dialect.escapechar)\n",
    "    print(\"Doublequote:\", dialect.doublequote)\n",
    "    print(\"Quote char:\", dialect.quotechar)\n",
    "    print(\"Line terminator:\", dialect.lineterminator)\n",
    "    print(\"Dialect quoting:\", dialect.quoting)\n",
    "    print(\"Skip initial space:\", dialect.skipinitialspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_csv) as infile:\n",
    "    filter_dict_csv = {}\n",
    "    rowreader = csv.reader(infile, delimiter=',')\n",
    "    for rownum, row in enumerate(rowreader):\n",
    "        filter_dict_csv[row[0]] = row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Mention Data Export                             b\n",
      "0             Mentions                   1 to 0 of 0\n",
      "1           Project Id                    1998247687\n",
      "2             Query Id                  [1999542744]\n",
      "3           Start Date  Sat Jun 30 23:00:00 UTC 2018\n",
      "4             End Date  Wed Aug 01 23:00:00 UTC 2018\n",
      "5               Search                           buy\n",
      "6         Author Group              Test author list\n",
      "7             Category                      Rudeness\n",
      "8             Location                          [uk]\n",
      "9            Page Type                     [twitter]\n",
      "10           Sentiment                    [negative]\n",
      "11                 Tag              Customer Service\n",
      "12           Xcategory                      Slowness\n",
      "13             Xdomain               www.youtube.com\n",
      "14           Xlocation                          [ie]\n",
      "15          Xpage Type              [news, facebook]\n"
     ]
    }
   ],
   "source": [
    "filters_df = pd.read_csv(filename_csv)\n",
    "print(filters_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/testjson.json\n"
     ]
    }
   ],
   "source": [
    "filename_json = os.path.join(file_dir, \"testjson.json\")\n",
    "print(filename_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'datetime': '1971-01-01T04:14:00', 'integer': 40, 'text': 'Chicago Reader', 'float': 1.0, 'boolean': True, 'date': '1971-01-01', 'time': '04:14:00'}, {'datetime': '1948-01-01T14:57:13', 'integer': 63, 'text': 'Chicago Sun-Times', 'float': 1.27, 'boolean': True, 'date': '1948-01-01', 'time': '14:57:13'}, {'datetime': '1920-01-01T00:00:00', 'integer': 164, 'text': 'Chicago Tribune', 'float': 41800000.01, 'boolean': False, 'date': '1920-01-01', 'time': '00:00:00'}, {'datetime': '', 'integer': '', 'text': 'This row has blanks', 'float': '', 'boolean': '', 'date': '', 'time': ''}, {'datetime': '', 'integer': '', 'text': 'Unicode! Σ', 'float': '', 'boolean': '', 'date': '', 'time': ''}]\n"
     ]
    }
   ],
   "source": [
    "with open(filename_json, 'r') as infile:\n",
    "    # the load method in the json module \n",
    "    data_struct = json.load(infile)\n",
    "    print(data_struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/iris.pickle\n"
     ]
    }
   ],
   "source": [
    "filename_pickle = os.path.join(file_dir, \"iris.pickle\")\n",
    "print(filename_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      NaN           NaN          NaN          NaN\n",
      "sepal_length  sepal_width  petal_length  petal_width      species\n",
      "5.1                   3.5           1.4          0.2  Iris-setosa\n",
      "4.9                     3           1.4          0.2  Iris-setosa\n",
      "4.7                   3.2           1.3          0.2  Iris-setosa\n",
      "4.6                   3.1           1.5          0.2  Iris-setosa\n",
      "5                     3.6           1.4          0.2  Iris-setosa\n",
      "5.4                   3.9           1.7          0.4  Iris-setosa\n",
      "4.6                   3.4           1.4          0.3  Iris-setosa\n",
      "5                     3.4           1.5          0.2  Iris-setosa\n",
      "4.4                   2.9           1.4          0.2  Iris-setosa\n",
      "4.9                   3.1           1.5          0.1  Iris-setosa\n",
      "5.4                   3.7           1.5          0.2  Iris-setosa\n",
      "4.8                   3.4           1.6          0.2  Iris-setosa\n",
      "4.8                     3           1.4          0.1  Iris-setosa\n",
      "4.3                     3           1.1          0.1  Iris-setosa\n",
      "5.8                     4           1.2          0.2  Iris-setosa\n",
      "5.7                   4.4           1.5          0.4  Iris-setosa\n",
      "5.4                   3.9           1.3          0.4  Iris-setosa\n",
      "5.1                   3.5           1.4          0.3  Iris-setosa\n",
      "5.7                   3.8           1.7          0.3  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "with open(filename_pickle, mode='rb') as infile:\n",
    "    unpickledtable = pickle.load(infile)\n",
    "print(unpickledtable[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that turns an excel into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ludovica/LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/example_filters.xlsx\n"
     ]
    }
   ],
   "source": [
    "filename_xlsx = os.path.join(file_dir, \"example_filters.xlsx\")\n",
    "print(filename_xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function that reads the workbook and puts the data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_dict(filename):\n",
    "    spreadsheet = (load_workbook(filename))\n",
    "    active_sheet = spreadsheet.active\n",
    "    data = active_sheet.values\n",
    "    data_dict = {}\n",
    "    for row in data:\n",
    "        data_dict[row[0]] = row[1]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with some files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Author Group': 'Test author list',\n",
       " 'Category': 'Rudeness',\n",
       " 'End Date': 'Wed Aug 01 23:00:00 UTC 2018',\n",
       " 'Location': '[uk]',\n",
       " 'Mention Data Export': None,\n",
       " 'Mentions': '1 to 0 of 0',\n",
       " 'Page Type': '[twitter]',\n",
       " 'Project Id': '1998247687',\n",
       " 'Query Id': '[1999542744]',\n",
       " 'Search': 'buy',\n",
       " 'Sentiment': '[negative]',\n",
       " 'Start Date': 'Sat Jun 30 23:00:00 UTC 2018',\n",
       " 'Tag': 'Customer Service',\n",
       " 'Xcategory': 'Slowness',\n",
       " 'Xdomain': 'www.youtube.com',\n",
       " 'Xlocation': '[ie]',\n",
       " 'Xpage Type': '[news, facebook]'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_excel_file = os.path.join(file_dir, \"example_filters.xlsx\")\n",
    "\n",
    "excel_to_dict(my_excel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing more files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Example filters - ADAS.xlsx', 'Example filters - British Museum.xlsx', 'Example filters - Charter.xlsx', 'Example filters - Delta.xlsx', 'Example filters - Essity.xlsx', 'Example filters - Nokia.xlsx', 'Example filters - Peel Hunt.xlsx', 'Example filters - Rothschild.xlsx', 'Example filters - SUV.xlsx']\n"
     ]
    }
   ],
   "source": [
    "LZ_HOME = os.path.expanduser('~')\n",
    "file_dir = os.path.join(LZ_HOME, 'LZ Projects/python_exercises/exercises/02 read_write_data/datafiles/example_filters/')\n",
    "my_excel_file = os.path.join(file_dir, \"Example filters - ADAS.xlsx\")\n",
    "list_of_files = os.listdir(file_dir)\n",
    "print(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excel_to_dict(filename):\n",
    "    file_path = os.path.join(file_dir, filename)\n",
    "    spreadsheet = (load_workbook(file_path))\n",
    "    active_sheet = spreadsheet.active\n",
    "    data = active_sheet.values\n",
    "    data_dict = {}\n",
    "    for row in data:\n",
    "        data_dict[row[0]] = row[1]\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of applying the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example filters - ADAS.xlsx \n",
      "\n",
      " {'Chart Data Export': '', 'Xtag': 'ADAs/Law Enforcement,BW RS Exclude,Sales,Toyota Authors', 'Project Id': '1998216779', 'Xsite Group': 'Sales Sites,YouTube', 'negative': 0, None: None, 'Dimension 1': 'categories', 'Query Id': '[1998992822, 1998992692, 1998992773, 1998991973, 1998992755, 1998992547, 1998990352, 1998992913, 1998992813, 1998992874, 1998992778, 1998992763]', 'categories': '86', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'positive': 0, 'neutral': 0, 'Xdomain': 'www.youtube.com', 'Start Date': 'Mon Jan 01 05:00:00 UTC 2018', 'Xauthor Group': 'Sales Authors', 'End Date': 'Sun Apr 01 04:00:00 UTC 2018', 'Tag': 'Consumer posts', 'Dimension 2': 'sentiment'} \n",
      "\n",
      "Example filters - British Museum.xlsx \n",
      "\n",
      " {'BM RT': 0, 'Tate Owned': 0, 'Chart Data Export': '', 'Shop exclude': 0, 'Project Id': '1998229525', None: None, 'Dimension 1': 'weeks', 'Query Id': '[1999205209, 1999205212, 1999205215, 1999205202]', 'Aggregate': 'twitterRetweets', 'Xpage Type': '[facebook]', 'V&A Owned': 0, 'Met Owned': 0, 'British Museum Owned': 0, 'Xdomain': 'www.youtube.com', 'Start Date': 'Sun Jan 01 00:00:00 UTC 2017', 'weeks': '2016-12-26 00:00:00.0', 'Xtag': 'Shop exclude', 'End Date': 'Mon Jan 01 00:00:00 UTC 2018', 'Dimension 2': 'tags', 'British Museum Shop General': 0} \n",
      "\n",
      "Example filters - Charter.xlsx \n",
      "\n",
      " {'queries': 'CC - Amazon', 'Project Id': '1998228247', 'Author Type': 195, 'Brand Perception': 168, 'Dimension 1': 'queries', 'Sentiment': 0, 'Message Type': 193, 'AA - Relevent': 196, 'End Date': 'Sat Dec 16 05:00:00 UTC 2017', 'Xtag': 'Brand query exclusions', 'Topics2': 196, 'Topics': 94, None: None, 'Query Id': '[1999186472, 1999186522, 1999186477]', 'Xdomain': 'www.youtube.com', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'Brand Cleaning': 0, 'Start Date': 'Fri Jan 01 05:00:00 UTC 2016', 'Parent Category': 'AA - Relevent', 'Dimension 2': 'parentCategories', 'Chart Data Export': '', 'Emotion': 195} \n",
      "\n",
      "Example filters - Delta.xlsx \n",
      "\n",
      " {'Chart Data Export': '', 'Customer service': 0, 'queries': 'Travel for business', 'Project Id': '1998245961', 'Search': 'NOT RT', 'Meals & snacks': 0, 'Account Type': '[individual]', 'Dimension 1': 'queries', 'Query Id': '[1999491226]', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'Inflight entertainment': 0, 'Xdomain': 'www.youtube.com', 'Start Date': 'Sat Jul 01 04:00:00 UTC 2017', 'Value/Price': 0, 'Dimension 2': 'categories', 'End Date': 'Wed Aug 01 04:00:00 UTC 2018', 'Loyalty/Rewards': 0, 'Xtag': 'Corporate Twitter accounts', None: None} \n",
      "\n",
      "Example filters - Essity.xlsx \n",
      "\n",
      " {'Chart Data Export': '', 'Project Id': '1998248700', None: None, 'Dimension 1': 'days', 'Query Id': '[1999545653]', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'Essity- Business Bathroom needs': 59, 'Xdomain': 'www.youtube.com', 'Start Date': 'Sun Dec 31 23:00:00 UTC 2017', 'Xtag': 'Exclusions,Irrelevant to Essity,ToiletPaper- Not Public', 'End Date': 'Fri Aug 31 22:00:00 UTC 2018', 'Dimension 2': 'queries', 'days': '2017-12-31 23:00:00.0'} \n",
      "\n",
      "Example filters - Nokia.xlsx \n",
      "\n",
      " {'Chart Data Export': '', 'Project Id': '1998248269', 'Search': 'nokia', None: None, 'Mobile World Conference': 345, 'Dimension 1': 'weeks', 'Query Id': '[1999536803]', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'Xdomain': 'www.youtube.com', 'Start Date': 'Mon Jan 01 00:00:00 UTC 2018', 'Dimension 2': 'queries', 'End Date': 'Mon Apr 30 23:00:00 UTC 2018', 'weeks': '2018-01-01 00:00:00.0'} \n",
      "\n",
      "Example filters - Peel Hunt.xlsx \n",
      "\n",
      " {'Chart Data Export': '', '[BW] Superdry 2': 0, 'Xtag': '[BW] PH Exclusions', 'Project Id': '1998231967', '[BW] Fat Face 2': 14001, None: None, '[BW] Jack Wills 2': 0, 'Dimension 1': 'queries', 'Query Id': '[1999244266, 1999243736, 1999244284, 1999244291, 1999244295]', 'Aggregate': 'volume', '[BW] Ted Baker 2': 0, 'Xpage Type': '[facebook]', 'queries': '[BW] Fat Face 2', 'Xdomain': 'www.youtube.com', 'Start Date': 'Wed Feb 01 00:00:00 UTC 2017', 'Xauthor Group': 'Authors Exclusions', 'End Date': 'Thu Feb 01 00:00:00 UTC 2018', 'Dimension 2': 'queries', '[BW] Joules 2': 0} \n",
      "\n",
      "Example filters - Rothschild.xlsx \n",
      "\n",
      " {'[S&I] Crown Royal': 797566, 'queryGroups': '10 brands', 'Project Id': '1998246280', 'Dimension 1': 'queryGroups', \"[S&I] Dewar's\": 102538, 'Xdomain': 'www.youtube.com', 'End Date': 'Wed Aug 01 04:00:00 UTC 2018', 'Dimension 2': 'queries', '[S&I] Svedka': 154217, '[S&I] Smirnoff': 1437052, '[S&I] Bacardi': 1072317, \"[S&I] Tito's\": 486614, '[S&I] Johnnie Walker': 296644, None: None, '[S&I] Captain Morgan': 21, 'Query Id': '[1999489407, 1999489385, 1999489416, 1999489400, 1999489419, 1999489387, 1999489412, 1999489382, 1999489408, 1999489395]', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', '[S&I] Ketel One': 121136, 'Start Date': 'Thu Aug 01 04:00:00 UTC 2013', 'Xtag': 'Irrelevant', 'Chart Data Export': '', \"[S&I] Jack Daniel's\": 162} \n",
      "\n",
      "Example filters - SUV.xlsx \n",
      "\n",
      " {'Location': '[us]', 'Chart Data Export': '', 'Xtag': 'Exclusions', 'Project Id': '1998230778', None: None, 'Category: Small SUV': 0, 'Dimension 1': 'queries', 'Query Id': '[1999223818, 1999223297]', 'Aggregate': 'volume', 'Xpage Type': '[facebook]', 'queries': 'Category: Sedan', 'Xdomain': 'www.youtube.com', 'Start Date': 'Sun Jan 01 05:00:00 UTC 2012', 'Xauthor Group': 'Exclusions: Author list', 'End Date': 'Mon Jan 01 05:00:00 UTC 2018', 'Category: Sedan': 14, 'Dimension 2': 'queries', 'Language': '[en]'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in list_of_files:\n",
    "    print(file, \"\\n\\n\", excel_to_dict(file), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
